{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "564f83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from aggregator import *\n",
    "from graphsage import *\n",
    "from encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3bc709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/graph.json\") as g:\n",
    "    graph = json.load(g)\n",
    "\n",
    "x, adj_lists, y = [], {}, []\n",
    "\n",
    "a = 0\n",
    "for muni_id, dta in graph.items():\n",
    "    x.append(dta[\"x\"])\n",
    "    y.append(dta[\"label\"])\n",
    "    adj_lists[str(a)] = dta[\"neighbors\"]\n",
    "    a += 1\n",
    "    \n",
    "x = np.array(x)\n",
    "y = np.expand_dims(np.array(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c00e1c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rapids/notebooks/sciclone/geograd/Heather/borderGraph/encoder.py:37: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight1)\n"
     ]
    }
   ],
   "source": [
    "agg = MeanAggregator(features = x,\n",
    "                    gcn = False)\n",
    "enc = Encoder(features = x, \n",
    "              feature_dim = x.shape[1], \n",
    "              embed_dim = 128, \n",
    "              adj_lists = adj_lists,\n",
    "              aggregator = agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d466580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rapids/notebooks/sciclone/geograd/Heather/borderGraph/graphsage.py:21: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n"
     ]
    }
   ],
   "source": [
    "model = SupervisedGraphSage(num_classes = 1,\n",
    "                            enc = enc)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d94d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64 # batch_size\n",
    "\n",
    "train_num = int(x.shape[0] * .70)\n",
    "train_indices = random.sample(range(0, x.shape[0]), train_num)\n",
    "val_indices = [i for i in range(0, x.shape[0]) if i not in train_indices]\n",
    "\n",
    "train_indices_b = [train_indices[i * n:(i + 1) * n] for i in range((len(train_indices) + n - 1) // n )] \n",
    "val_indices_b   = [val_indices[i * n:(i + 1) * n] for i in range((len(val_indices) + n - 1) // n )] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0647d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Training Loss:  30.17127169744932  Validation Loss:  26.133861418693296\n",
      "Epoch:  1 Training Loss:  29.102245452163295  Validation Loss:  25.433693227460306\n",
      "Epoch:  2 Training Loss:  28.469685408089667  Validation Loss:  25.104756902879284\n",
      "Epoch:  3 Training Loss:  27.76006970333031  Validation Loss:  24.438416807113157\n",
      "Epoch:  4 Training Loss:  26.575773922404476  Validation Loss:  23.78987480901903\n",
      "Epoch:  5 Training Loss:  25.345523311884076  Validation Loss:  23.28914578345514\n",
      "Epoch:  6 Training Loss:  24.56903540478043  Validation Loss:  22.850752996629286\n",
      "Epoch:  7 Training Loss:  24.054647654088864  Validation Loss:  22.499888315508443\n",
      "Epoch:  8 Training Loss:  23.662631761648512  Validation Loss:  22.142512955204133\n",
      "Epoch:  9 Training Loss:  23.335095839546593  Validation Loss:  21.81769079392956\n",
      "Epoch:  10 Training Loss:  23.04156958446793  Validation Loss:  21.52873274280179\n",
      "Epoch:  11 Training Loss:  22.772364150768823  Validation Loss:  21.30716365691154\n",
      "Epoch:  12 Training Loss:  22.557491867044323  Validation Loss:  21.12704103531376\n",
      "Epoch:  13 Training Loss:  22.360187831261346  Validation Loss:  20.921883072391633\n",
      "Epoch:  14 Training Loss:  22.16149539340746  Validation Loss:  20.76562263734879\n",
      "Epoch:  15 Training Loss:  22.004711778991616  Validation Loss:  20.614663499401463\n",
      "Epoch:  16 Training Loss:  21.846636633655343  Validation Loss:  20.463733697706655\n",
      "Epoch:  17 Training Loss:  21.7120608254587  Validation Loss:  20.39991263112714\n",
      "Epoch:  18 Training Loss:  21.554365271519494  Validation Loss:  20.30806899532195\n",
      "Epoch:  19 Training Loss:  21.428213961061765  Validation Loss:  20.262263341103832\n",
      "Epoch:  20 Training Loss:  21.28464462681265  Validation Loss:  20.158358370873234\n",
      "Epoch:  21 Training Loss:  21.18924355407968  Validation Loss:  20.068243998865928\n",
      "Epoch:  22 Training Loss:  21.056548903427018  Validation Loss:  20.026871465867565\n",
      "Epoch:  23 Training Loss:  20.906562604679937  Validation Loss:  20.04993414109753\n",
      "Epoch:  24 Training Loss:  20.805415236603668  Validation Loss:  19.89603950746598\n",
      "Epoch:  25 Training Loss:  20.668455089606024  Validation Loss:  19.73266109343498\n",
      "Epoch:  26 Training Loss:  20.60333977959133  Validation Loss:  19.746529413038683\n",
      "Epoch:  27 Training Loss:  20.453522446079702  Validation Loss:  19.831944521011845\n",
      "Epoch:  28 Training Loss:  20.450126299719592  Validation Loss:  19.5206053703062\n",
      "Epoch:  29 Training Loss:  20.33792266212558  Validation Loss:  19.52894292031565\n",
      "Epoch:  30 Training Loss:  20.23286237492436  Validation Loss:  19.44524678876323\n",
      "Epoch:  31 Training Loss:  20.20546430110272  Validation Loss:  19.439879681987147\n",
      "Epoch:  32 Training Loss:  20.082378128917057  Validation Loss:  19.52719460764239\n",
      "Epoch:  33 Training Loss:  19.984736230198607  Validation Loss:  19.28073228405368\n",
      "Epoch:  34 Training Loss:  19.90535426898286  Validation Loss:  19.234756174395162\n",
      "Epoch:  35 Training Loss:  19.783048299997837  Validation Loss:  19.24979518767326\n",
      "Epoch:  36 Training Loss:  19.667840770336273  Validation Loss:  19.128657974735383\n",
      "Epoch:  37 Training Loss:  19.640800929827975  Validation Loss:  18.92139636624244\n",
      "Epoch:  38 Training Loss:  19.579799372460666  Validation Loss:  19.00685734902659\n",
      "Epoch:  39 Training Loss:  19.47180977764631  Validation Loss:  18.95670879733178\n",
      "Epoch:  40 Training Loss:  19.408694614183524  Validation Loss:  18.862952300040952\n",
      "Epoch:  41 Training Loss:  19.41300569695226  Validation Loss:  18.68996842907321\n",
      "Epoch:  42 Training Loss:  19.3093638650939  Validation Loss:  18.696553039550782\n",
      "Epoch:  43 Training Loss:  19.165016058239864  Validation Loss:  18.736080981839088\n",
      "Epoch:  44 Training Loss:  19.08744543980431  Validation Loss:  18.58795658234627\n",
      "Epoch:  45 Training Loss:  19.01181431265128  Validation Loss:  18.563485816217238\n",
      "Epoch:  46 Training Loss:  19.056060959854232  Validation Loss:  18.558251214796496\n",
      "Epoch:  47 Training Loss:  18.830169213428206  Validation Loss:  18.45893790952621\n",
      "Epoch:  48 Training Loss:  18.817782158014026  Validation Loss:  18.309346648185485\n",
      "Epoch:  49 Training Loss:  18.773167189390993  Validation Loss:  18.373453694005168\n",
      "Epoch:  50 Training Loss:  18.735530700261496  Validation Loss:  18.34306128717238\n",
      "Epoch:  51 Training Loss:  18.65345065590422  Validation Loss:  18.251422857469127\n",
      "Epoch:  52 Training Loss:  18.49360321171551  Validation Loss:  18.279341519263482\n",
      "Epoch:  53 Training Loss:  18.476782116817407  Validation Loss:  18.227526067918348\n",
      "Epoch:  54 Training Loss:  18.51458478534535  Validation Loss:  18.190784528178554\n",
      "Epoch:  55 Training Loss:  18.44893564986655  Validation Loss:  18.154571139427922\n",
      "Epoch:  56 Training Loss:  18.30302401762945  Validation Loss:  18.043519690728957\n",
      "Epoch:  57 Training Loss:  18.18241149986763  Validation Loss:  18.069107646326867\n",
      "Epoch:  58 Training Loss:  18.212334639468793  Validation Loss:  18.04735821139428\n",
      "Epoch:  59 Training Loss:  18.128040403416215  Validation Loss:  17.957586866809475\n",
      "Epoch:  60 Training Loss:  18.032289876977437  Validation Loss:  17.942510395665323\n",
      "Epoch:  61 Training Loss:  17.92353624525901  Validation Loss:  17.961707281297254\n",
      "Epoch:  62 Training Loss:  17.905525513539505  Validation Loss:  17.764015493085306\n",
      "Epoch:  63 Training Loss:  17.844472333768262  Validation Loss:  17.762136643932713\n",
      "Epoch:  64 Training Loss:  17.77963556524464  Validation Loss:  17.75713377921812\n",
      "Epoch:  65 Training Loss:  17.719689375796918  Validation Loss:  17.718948708811112\n",
      "Epoch:  66 Training Loss:  17.613771472893976  Validation Loss:  17.576307284447456\n",
      "Epoch:  67 Training Loss:  17.600336182991335  Validation Loss:  17.611399004536292\n",
      "Epoch:  68 Training Loss:  17.570985237427603  Validation Loss:  17.61113054829259\n",
      "Epoch:  69 Training Loss:  17.4784805487795  Validation Loss:  17.502567315870717\n",
      "Epoch:  70 Training Loss:  17.45407505483878  Validation Loss:  17.434830499464464\n",
      "Epoch:  71 Training Loss:  17.357966295086662  Validation Loss:  17.400945897256175\n",
      "Epoch:  72 Training Loss:  17.257911819310273  Validation Loss:  17.330521072879915\n",
      "Epoch:  73 Training Loss:  17.21706087526609  Validation Loss:  17.319268552718626\n",
      "Epoch:  74 Training Loss:  17.100226395687457  Validation Loss:  17.377967686806954\n",
      "Epoch:  75 Training Loss:  17.067828884098375  Validation Loss:  17.244250832834553\n",
      "Epoch:  76 Training Loss:  16.990042933125054  Validation Loss:  17.37402580015121\n",
      "Epoch:  77 Training Loss:  16.870545573254343  Validation Loss:  17.24864295221144\n",
      "Epoch:  78 Training Loss:  16.831897403194695  Validation Loss:  17.07659857965285\n",
      "Epoch:  79 Training Loss:  16.84620098784092  Validation Loss:  17.131629796181954\n",
      "Epoch:  80 Training Loss:  16.707234742730485  Validation Loss:  17.074312173166582\n",
      "Epoch:  81 Training Loss:  16.62449276661642  Validation Loss:  16.988089678364414\n",
      "Epoch:  82 Training Loss:  16.632552699594246  Validation Loss:  16.90137698265814\n",
      "Epoch:  83 Training Loss:  16.52051990556519  Validation Loss:  16.834228909400203\n",
      "Epoch:  84 Training Loss:  16.41037835718685  Validation Loss:  17.00499484154486\n",
      "Epoch:  85 Training Loss:  16.52031997955027  Validation Loss:  16.869197820848033\n",
      "Epoch:  86 Training Loss:  16.363274074359225  Validation Loss:  16.883994760820944\n",
      "Epoch:  87 Training Loss:  16.354677635604414  Validation Loss:  16.907071562736267\n",
      "Epoch:  88 Training Loss:  16.211703921749383  Validation Loss:  16.95951380575857\n",
      "Epoch:  89 Training Loss:  16.237523312390593  Validation Loss:  16.744892341859877\n",
      "Epoch:  90 Training Loss:  16.177440405881256  Validation Loss:  16.837015262726815\n",
      "Epoch:  91 Training Loss:  16.06004352490437  Validation Loss:  16.7947747507403\n",
      "Epoch:  92 Training Loss:  15.960665564319404  Validation Loss:  16.899227609942038\n",
      "Epoch:  93 Training Loss:  15.944114590086878  Validation Loss:  16.81390887844947\n",
      "Epoch:  94 Training Loss:  15.920102785566865  Validation Loss:  16.662371826171874\n",
      "Epoch:  95 Training Loss:  15.78201321381586  Validation Loss:  16.623763004426035\n",
      "Epoch:  96 Training Loss:  15.840916550505705  Validation Loss:  16.638497088032384\n",
      "Epoch:  97 Training Loss:  15.751960532793861  Validation Loss:  16.811748578471523\n",
      "Epoch:  98 Training Loss:  15.814641274522119  Validation Loss:  16.995350794638355\n",
      "Epoch:  99 Training Loss:  15.885692084643521  Validation Loss:  16.721147598758822\n",
      "Epoch:  100 Training Loss:  15.82839575381035  Validation Loss:  16.514582184822327\n",
      "Epoch:  101 Training Loss:  15.68945040247747  Validation Loss:  16.393916714575983\n",
      "Epoch:  102 Training Loss:  15.532090436373508  Validation Loss:  16.40252739690965\n",
      "Epoch:  103 Training Loss:  15.505190584174825  Validation Loss:  16.354322322722403\n",
      "Epoch:  104 Training Loss:  15.477788964742446  Validation Loss:  16.418895253827493\n",
      "Epoch:  105 Training Loss:  15.470822029588629  Validation Loss:  16.445252498503653\n",
      "Epoch:  106 Training Loss:  15.409214468252939  Validation Loss:  16.51749484154486\n",
      "Epoch:  107 Training Loss:  15.520604810279435  Validation Loss:  16.423016455865675\n",
      "Epoch:  108 Training Loss:  15.436875297155947  Validation Loss:  16.38204793622417\n",
      "Epoch:  109 Training Loss:  15.228641156503933  Validation Loss:  16.32463127874559\n",
      "Epoch:  110 Training Loss:  15.300812442932552  Validation Loss:  16.443692262711064\n",
      "Epoch:  111 Training Loss:  15.19695137414365  Validation Loss:  16.27428515034337\n",
      "Epoch:  112 Training Loss:  15.116031483166116  Validation Loss:  16.268693739368068\n",
      "Epoch:  113 Training Loss:  15.1943476295735  Validation Loss:  16.315261545488912\n",
      "Epoch:  114 Training Loss:  15.179552450219624  Validation Loss:  16.22804137199156\n",
      "Epoch:  115 Training Loss:  15.126944218763507  Validation Loss:  16.28149177797379\n",
      "Epoch:  116 Training Loss:  15.034619174392722  Validation Loss:  16.155058780793222\n",
      "Epoch:  117 Training Loss:  14.977856141403008  Validation Loss:  16.08094772831086\n",
      "Epoch:  118 Training Loss:  14.896868841611829  Validation Loss:  16.14417301301033\n",
      "Epoch:  119 Training Loss:  14.83812231377777  Validation Loss:  15.933797577888734\n",
      "Epoch:  120 Training Loss:  14.805442715086878  Validation Loss:  16.038705050560736\n",
      "Epoch:  121 Training Loss:  14.790468946696977  Validation Loss:  16.08284464190083\n",
      "Epoch:  122 Training Loss:  14.758303545982182  Validation Loss:  15.974166624007687\n",
      "Epoch:  123 Training Loss:  14.71114837941952  Validation Loss:  16.075138116651967\n",
      "Epoch:  124 Training Loss:  14.65126824807659  Validation Loss:  15.961255621141003\n",
      "Epoch:  125 Training Loss:  14.813713295331139  Validation Loss:  16.120005207677043\n",
      "Epoch:  126 Training Loss:  14.609216544967799  Validation Loss:  15.756725286668347\n",
      "Epoch:  127 Training Loss:  14.609588179845533  Validation Loss:  15.949164015246977\n",
      "Epoch:  128 Training Loss:  14.611805473950229  Validation Loss:  15.91843965591923\n",
      "Epoch:  129 Training Loss:  14.533812442426035  Validation Loss:  15.864927870227445\n",
      "Epoch:  130 Training Loss:  14.434417745714201  Validation Loss:  15.837545431813886\n",
      "Epoch:  131 Training Loss:  14.428168459057312  Validation Loss:  15.738480401808216\n",
      "Epoch:  132 Training Loss:  14.44755965743322  Validation Loss:  15.887515849451866\n",
      "Epoch:  133 Training Loss:  14.527430300890657  Validation Loss:  16.33250250047253\n",
      "Epoch:  134 Training Loss:  14.344307547288317  Validation Loss:  15.77566882717994\n",
      "Epoch:  135 Training Loss:  14.313700400282569  Validation Loss:  15.835031472482989\n",
      "Epoch:  136 Training Loss:  14.236522078349218  Validation Loss:  15.840177523705266\n",
      "Epoch:  137 Training Loss:  14.25945994270293  Validation Loss:  15.764039907147808\n",
      "Epoch:  138 Training Loss:  14.246973420244695  Validation Loss:  15.823681492959299\n",
      "Epoch:  139 Training Loss:  14.208004646776129  Validation Loss:  15.764768637380293\n",
      "Epoch:  140 Training Loss:  14.146032963880694  Validation Loss:  15.590046938004033\n",
      "Epoch:  141 Training Loss:  14.101088021306081  Validation Loss:  15.81959730579007\n",
      "Epoch:  142 Training Loss:  14.171624063621088  Validation Loss:  15.795758696525327\n",
      "Epoch:  143 Training Loss:  14.045722127785162  Validation Loss:  15.539486300560736\n",
      "Epoch:  144 Training Loss:  14.065282776807518  Validation Loss:  15.608987475979713\n",
      "Epoch:  145 Training Loss:  13.98074433681546  Validation Loss:  15.486520385742187\n",
      "Epoch:  146 Training Loss:  13.937044853326526  Validation Loss:  15.831818217615927\n",
      "Epoch:  147 Training Loss:  13.79505538412809  Validation Loss:  15.635201829479586\n",
      "Epoch:  148 Training Loss:  13.882985918353702  Validation Loss:  15.728222065587197\n",
      "Epoch:  149 Training Loss:  13.791897848928619  Validation Loss:  15.5225339827999\n",
      "Epoch:  150 Training Loss:  13.840052603356241  Validation Loss:  15.444207074565272\n",
      "Epoch:  151 Training Loss:  13.94004114765679  Validation Loss:  15.66432874125819\n",
      "Epoch:  152 Training Loss:  13.902456914076007  Validation Loss:  15.467240659652218\n",
      "Epoch:  153 Training Loss:  14.039643178177407  Validation Loss:  15.338851338048135\n",
      "Epoch:  154 Training Loss:  14.015033811619338  Validation Loss:  15.545987036920362\n",
      "Epoch:  155 Training Loss:  13.681453847291559  Validation Loss:  15.412535242880544\n",
      "Epoch:  156 Training Loss:  13.802280600140204  Validation Loss:  15.73792503110824\n",
      "Epoch:  157 Training Loss:  13.699357239866982  Validation Loss:  15.429426820816532\n",
      "Epoch:  158 Training Loss:  13.719595100540014  Validation Loss:  15.43770737186555\n",
      "Epoch:  159 Training Loss:  13.662781690331085  Validation Loss:  15.444400664298765\n",
      "Epoch:  160 Training Loss:  13.663088617812878  Validation Loss:  15.670249692855343\n",
      "Epoch:  161 Training Loss:  13.643545232538036  Validation Loss:  15.60640165267452\n",
      "Epoch:  162 Training Loss:  13.580620519023384  Validation Loss:  15.477810668945313\n",
      "Epoch:  163 Training Loss:  13.516623300470586  Validation Loss:  15.75382326187626\n",
      "Epoch:  164 Training Loss:  13.66194769092945  Validation Loss:  15.758145043157763\n",
      "Epoch:  165 Training Loss:  13.621028451668936  Validation Loss:  15.707089627173639\n",
      "Epoch:  166 Training Loss:  13.657758530739432  Validation Loss:  15.52640617124496\n",
      "Epoch:  167 Training Loss:  13.480898233205286  Validation Loss:  15.631087813838835\n",
      "Epoch:  168 Training Loss:  13.476240334833971  Validation Loss:  15.489351481776083\n",
      "Epoch:  169 Training Loss:  13.516779687229858  Validation Loss:  15.943399933845766\n",
      "Epoch:  170 Training Loss:  13.718710027459911  Validation Loss:  16.004703152564264\n",
      "Epoch:  171 Training Loss:  13.556769659905019  Validation Loss:  15.703130512852823\n",
      "Epoch:  172 Training Loss:  13.28041874422572  Validation Loss:  15.261204971805695\n",
      "Epoch:  173 Training Loss:  13.192058093650047  Validation Loss:  15.618247395177042\n",
      "Epoch:  174 Training Loss:  13.23383425215303  Validation Loss:  15.381205601846018\n",
      "Epoch:  175 Training Loss:  13.131798202071447  Validation Loss:  15.348636258033014\n",
      "Epoch:  176 Training Loss:  13.347899166545103  Validation Loss:  15.417561340332032\n",
      "Epoch:  177 Training Loss:  13.398635062274431  Validation Loss:  15.437075953329764\n",
      "Epoch:  178 Training Loss:  13.144127388053251  Validation Loss:  15.47074205952306\n",
      "Epoch:  179 Training Loss:  13.33595765510865  Validation Loss:  15.245480740454889\n",
      "Epoch:  180 Training Loss:  13.19958067665786  Validation Loss:  15.02759502780053\n",
      "Epoch:  181 Training Loss:  13.229223375333625  Validation Loss:  15.097444448163433\n",
      "Epoch:  182 Training Loss:  13.423240988745894  Validation Loss:  15.274315717143397\n",
      "Epoch:  183 Training Loss:  13.305466985636562  Validation Loss:  14.916085224766885\n",
      "Epoch:  184 Training Loss:  13.400136755050301  Validation Loss:  14.968758958385836\n",
      "Epoch:  185 Training Loss:  13.183375927092788  Validation Loss:  14.836511427356351\n",
      "Epoch:  186 Training Loss:  13.172572852170319  Validation Loss:  14.812778989730342\n",
      "Epoch:  187 Training Loss:  13.298128858806352  Validation Loss:  14.86112776725523\n",
      "Epoch:  188 Training Loss:  13.226205997282374  Validation Loss:  14.84117677750126\n",
      "Epoch:  189 Training Loss:  13.162939638847467  Validation Loss:  14.874130470521989\n",
      "Epoch:  190 Training Loss:  13.107308394351605  Validation Loss:  15.067439639183783\n",
      "Epoch:  191 Training Loss:  13.036821969482084  Validation Loss:  14.797862735871346\n",
      "Epoch:  192 Training Loss:  12.82739922614513  Validation Loss:  14.826720133135396\n",
      "Epoch:  193 Training Loss:  12.915848453674739  Validation Loss:  14.713070924820439\n",
      "Epoch:  194 Training Loss:  12.955973328396492  Validation Loss:  14.78234875586725\n",
      "Epoch:  195 Training Loss:  12.700453192366604  Validation Loss:  14.772205106673702\n",
      "Epoch:  196 Training Loss:  12.690709769643359  Validation Loss:  14.70759028773154\n",
      "Epoch:  197 Training Loss:  12.59791662023605  Validation Loss:  14.817474586732926\n",
      "Epoch:  198 Training Loss:  12.830550873098202  Validation Loss:  14.711199385120024\n",
      "Epoch:  199 Training Loss:  12.71822366674906  Validation Loss:  14.674709369290259\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 200):\n",
    "    \n",
    "    running_train_loss, running_val_loss = 0, 0\n",
    "    \n",
    "    for batch in train_indices_b:\n",
    "        \n",
    "        model.train()\n",
    "                \n",
    "        batch_nodes = [str(i) for i in batch]\n",
    "        batch_ys = torch.tensor([y[int(i)] for i in batch])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(batch_nodes, batch_ys)\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    for batch in val_indices_b:\n",
    "        \n",
    "        model.eval()\n",
    "                \n",
    "        batch_nodes = [str(i) for i in batch]\n",
    "        batch_ys = torch.tensor([y[int(i)] for i in batch])\n",
    "\n",
    "        loss = model.loss(batch_nodes, batch_ys)\n",
    "        \n",
    "        running_val_loss += loss.item()\n",
    "        \n",
    "    t_loss = running_train_loss / len(train_indices)\n",
    "    v_loss = running_val_loss / len(val_indices)\n",
    "        \n",
    "    print(\"Epoch: \", epoch, \"Training Loss: \", t_loss, \" Validation Loss: \", v_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ecea439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  87.4780],\n",
       "        [  25.6122],\n",
       "        [  16.7415],\n",
       "        [  13.9004],\n",
       "        [  99.0445],\n",
       "        [ 153.3100],\n",
       "        [  56.2089],\n",
       "        [  47.8745],\n",
       "        [ 107.6732],\n",
       "        [ 124.8682],\n",
       "        [  73.5030],\n",
       "        [ 142.2534],\n",
       "        [  62.9483],\n",
       "        [  58.3465],\n",
       "        [ 129.6899],\n",
       "        [  79.9416],\n",
       "        [ 318.7852],\n",
       "        [  61.5948],\n",
       "        [ 102.2355],\n",
       "        [ 103.2169],\n",
       "        [  97.1065],\n",
       "        [  18.7762],\n",
       "        [ 110.2240],\n",
       "        [ 146.3541],\n",
       "        [  73.0353],\n",
       "        [ 126.9746],\n",
       "        [  36.9999],\n",
       "        [ 110.0958],\n",
       "        [ 103.7611],\n",
       "        [  53.2191],\n",
       "        [ 853.3050],\n",
       "        [ 566.6194],\n",
       "        [1088.6333],\n",
       "        [ 747.7380],\n",
       "        [3478.5649],\n",
       "        [1309.6831],\n",
       "        [3827.7168],\n",
       "        [  47.8296],\n",
       "        [1014.8962],\n",
       "        [ 955.1300],\n",
       "        [1619.0576],\n",
       "        [1080.9343],\n",
       "        [ 513.8354],\n",
       "        [1110.3741]], grad_fn=<TBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86fd88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e5c9adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "122\n",
      "126\n",
      "152\n",
      "153\n",
      "1608\n",
      "2027\n",
      "2043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>988.0</td>\n",
       "      <td>4377.952637</td>\n",
       "      <td>3389.952637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1446.0</td>\n",
       "      <td>1027.879883</td>\n",
       "      <td>418.120117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>150.970917</td>\n",
       "      <td>150.970917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>955.0</td>\n",
       "      <td>137.917358</td>\n",
       "      <td>817.082642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.0</td>\n",
       "      <td>118.052299</td>\n",
       "      <td>81.052299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>974.0</td>\n",
       "      <td>1405.406494</td>\n",
       "      <td>431.406494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1918.0</td>\n",
       "      <td>1608.859375</td>\n",
       "      <td>309.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>1551.0</td>\n",
       "      <td>1456.220337</td>\n",
       "      <td>94.779663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>717.0</td>\n",
       "      <td>615.752930</td>\n",
       "      <td>101.247070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>2487.0</td>\n",
       "      <td>775.376770</td>\n",
       "      <td>1711.623230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       true         pred     abs_diff\n",
       "0     988.0  4377.952637  3389.952637\n",
       "1    1446.0  1027.879883   418.120117\n",
       "2       0.0   150.970917   150.970917\n",
       "3     955.0   137.917358   817.082642\n",
       "4      37.0   118.052299    81.052299\n",
       "..      ...          ...          ...\n",
       "607   974.0  1405.406494   431.406494\n",
       "608  1918.0  1608.859375   309.140625\n",
       "609  1551.0  1456.220337    94.779663\n",
       "610   717.0   615.752930   101.247070\n",
       "611  2487.0   775.376770  1711.623230\n",
       "\n",
       "[612 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues, preds = [], []\n",
    "\n",
    "for index in val_indices:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        input = [str(index)]\n",
    "        output = torch.tensor(y[index])\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        loss = model.loss(input, output)\n",
    "\n",
    "        trues.append(y[index][0])\n",
    "        preds.append(model.scores.item())\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(index)\n",
    "    \n",
    "    \n",
    "preds_df = pd.DataFrame()\n",
    "preds_df['true'], preds_df['pred'] = trues, preds\n",
    "preds_df[\"abs_diff\"] = abs(preds_df['true'] - preds_df['pred'])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0a9891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv(\"./predictions/graph_preds_v2_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639866c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
